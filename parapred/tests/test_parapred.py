import unittest
import torch
from parapred.model import Parapred
from parapred.preprocessing import encode_parapred, encode_batch
from parapred.cnn import generate_mask


class ParapredTest(unittest.TestCase):
    def setUp(self):
        self.model = Parapred()
        self.model.load_state_dict(torch.load("../weights/parapred_pytorch.h5"))
        _ = self.model.eval()

        self.sequence = "CAKYPYYYGTSHWYFDVW"
        self.max_length = 40
        self.num_features = 28

    def test_probailities(self):
        encoding, lengths = encode_batch(["CAKYPYYYGTSHWYFDVW"], self.max_length)
        m = generate_mask(encoding, lengths)

        with torch.no_grad():
            pr = self.model(encoding, m, lengths)

        v = pr.view(1, -1)[0][:lengths[0].item()]

        self.assertTrue(
            torch.allclose(
                torch.Tensor([0.00617, 0.01301, 0.15066, 0.36306, 0.17887, 0.19109, 0.87802,
                              0.8394, 0.62666, 0.63762, 0.67999, 0.65898, 0.41061, 0.75221, 0.17845,
                              0.28998, 0.12861, 0.22138]), v,
                rtol=1e-2
            )
        )

    def test_encoding(self):
        """
        Test the encoding function
        """
        # Deliberately not testing padding
        encoded_representation = encode_parapred(self.sequence, len(self.sequence))

        self.assertTrue(
            torch.allclose(
                torch.Tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7700, 0.1300, 2.4300,
                               1.5400, 6.3500, 0.1700, 0.4100],
                              [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2800, 0.0500, 1.0000,
                               0.3100, 6.1100, 0.4200, 0.2300],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.8900, 0.2200, 4.7700,
                               -0.9900, 9.9900, 0.3200, 0.2700],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 2.9400, 0.3000, 6.4700,
                               0.9600, 5.6600, 0.2500, 0.4100],
                              [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6700, 0.0000, 2.7200,
                               0.7200, 6.8000, 0.1300, 0.3400],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 2.9400, 0.3000, 6.4700,
                               0.9600, 5.6600, 0.2500, 0.4100],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 2.9400, 0.3000, 6.4700,
                               0.9600, 5.6600, 0.2500, 0.4100],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 2.9400, 0.3000, 6.4700,
                               0.9600, 5.6600, 0.2500, 0.4100],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 6.0700, 0.1300, 0.1500],
                              [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0300, 0.1100, 2.6000,
                               0.2600, 5.6000, 0.2100, 0.3600],
                              [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.3100, 0.0600, 1.6000,
                               -0.0400, 5.7000, 0.2000, 0.2800],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.9900, 0.2300, 4.6600,
                               0.1300, 7.6900, 0.2700, 0.3000],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 3.2100, 0.4100, 8.0800,
                               2.2500, 5.9400, 0.3200, 0.4200],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 2.9400, 0.3000, 6.4700,
                               0.9600, 5.6600, 0.2500, 0.4100],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 2.9400, 0.2900, 5.8900,
                               1.7900, 5.6700, 0.3000, 0.3800],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6000, 0.1100, 2.7800,
                               -0.7700, 2.9500, 0.2500, 0.2000],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.6700, 0.1400, 3.0000,
                               1.2200, 6.0200, 0.2700, 0.4900],
                              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
                               0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 3.2100, 0.4100, 8.0800,
                               2.2500, 5.9400, 0.3200, 0.4200]]), encoded_representation
            )
        )

    def test_batch_prediction(self):
        batch = ["CAKYPYYYGTSHWYFDVW", "CSQSYNYPYTF"]
        encoding, lengths = encode_batch(batch, self.max_length)
        m = generate_mask(encoding, lengths)

        with torch.no_grad():
            pr = self.model(encoding, m, lengths)

        print(pr.view(len(batch),-1).shape)

        v1 = pr.view(len(batch), -1)[0][:lengths[0].item()]
        v2 = pr.view(len(batch), -1)[1][:lengths[1].item()]

        self.assertTrue(
            torch.allclose(
                torch.Tensor([0.00617, 0.01301, 0.15066, 0.36306, 0.17887, 0.19109, 0.87802,
                              0.8394, 0.62666, 0.63762, 0.67999, 0.65898, 0.41061, 0.75221, 0.17845,
                              0.28998, 0.12861, 0.22138]), v1,
                rtol=1e-2
            )
        )
        self.assertTrue(
            torch.allclose(
                torch.Tensor([0.00422, 0.04747, 0.35151, 0.41757, 0.53621, 0.83283, 0.78089, 0.93526,
                              0.71154, 0.81395, 0.17341]), v2,
                rtol=1e-2
            )
        )